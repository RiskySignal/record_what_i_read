# 针对大模型的攻击

[TOC]

## 数据泄露

### 训练数据泄露

###### Scalable Extraction of Training Data from (Production) Language Models

链接：https://arxiv.org/pdf/2311.17035v1.pdf

<u>【非常棒】Carlini大佬做得实验非常扎实，是usenix文章的续作，从完全开源的LLM，到部分开源的LLM，再到RLHF后的ChatGPT模型都进行了讨论。作者的方法可以验证模型生成的内容中记忆内容的占比。基座模型的训练数据验证，可以参考这篇文章。隐私验证的第一环也可以用这篇文章的方法。</u>

<img src="pictures/image-20231129215329375.png" alt="image-20231129215329375" style="zoom: 25%;" />



## 广义对抗攻击

### 综合

###### OpenAI安全系统负责人长文梳理：大模型的对抗攻击与防御

公众号链接：https://mp.weixin.qq.com/s/t87IOi6r4N-c-StI9CPy_A



### 恶意内容生成

#### 文生文

###### Universal and Transferable Adversarial Attacks on Aligned Language Models

链接：https://arxiv.org/abs/2307.15043

GitHub链接：https://github.com/llm-attacks/llm-attacks



#### 图生文

###### Query-Relevant Images Jailbreak Large Multi-Modal Models

链接：https://arxiv.org/pdf/2311.17600v1.pdf

GitHub链接：https://github.com/isXinLiu/MM-SafetyBench

<u>【文章不行，不建议阅读】</u>

就是把以前有问题的指令搞进图像，然后做了个benchmark，<u>你说这样的工作有啥意思。</u>

<img src="pictures/image-20231208192422464.png" alt="image-20231208192422464" style="zoom:33%;" />



## 后门攻击

### 综合



### RLHF阶段

###### Universal Jailbreak Backdoors from Poisoned Human Feedback

链接：https://arxiv.org/abs/2311.14455v1

<u>【文章不行，不建议阅读】</u>

1. 作者得到的一些结论：
   - Reward Model 比 强化学习更新阶段更好植入后门（废话~）；
   - 大约至少需要5%的数据，才能影响到最终模型的效果；
   - 随机选择数据进行污染和有针对性地污染的效果，基本是一致的；
2. 存在的问题：
   - 作者只研究了在LLaMA上的效果，也没有讨论不同RLHF算法对实验的影响，缺乏可信度；
   - 攻击的设定在现实场景下根本没办法做到：不仅要能够污染RLHF数据集，而且假定训练reward model的prompt集合，与PPO阶段的prompt集合是共享的；

