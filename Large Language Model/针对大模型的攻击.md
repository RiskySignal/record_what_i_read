# 针对大模型的攻击

[TOC]

## 对抗攻击

### 综合

###### OpenAI安全系统负责人长文梳理：大模型的对抗攻击与防御

公众号链接：https://mp.weixin.qq.com/s/t87IOi6r4N-c-StI9CPy_A



### 恶意内容生成

#### 文生文





## 后门攻击

### 综合



### RLHF阶段

###### Universal Jailbreak Backdoors from Poisoned Human Feedback

链接：https://arxiv.org/abs/2311.14455v1

<u>【文章不怎么样】</u>

1. 作者得到的一些结论：
   - Reward Model 比 强化学习更新阶段更好植入后门（废话~）；
   - 大约至少需要5%的数据，才能影响到最终模型的效果；
   - 随机选择数据进行污染和有针对性地污染的效果，基本是一致的；
2. 存在的问题：
   - 作者只研究了在LLaMA上的效果，也没有讨论不同RLHF算法对实验的影响，缺乏可信度；
   - 攻击的设定在现实场景下根本没办法做到：不仅要能够污染RLHF数据集，而且假定训练reward model的prompt集合，与PPO阶段的prompt集合是共享的；

