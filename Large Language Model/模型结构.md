# 模型结构

[TOC]

## 开源模型

#### ChatGLM-3



#### LLaVA

论文链接：https://arxiv.org/abs/2304.08485

参考链接：

项目地址：https://github.com/haotian-liu/LLaVA



#### Monkey

论文链接：https://arxiv.org/abs/2311.06607

参考链接：https://zhuanlan.zhihu.com/p/669653695

项目地址：https://github.com/Yuliang-Liu/Monkey

华中科大的Monkey模型，主要为了解决两个问题：对高分辨率图像的解析，以及缺乏详细的描述数据；

解决的方法：

1. 保证训练计算量基本稳定的情况下，增大模型的图像分辨率；

<img src="pictures/image-20240130202555470.png" alt="image-20240130202555470" style="zoom: 33%;" />

2. 结合多级特征（如使用不同的模型识别图像位置、对图像进行切割等），利用GPT来生成详细的图像描述，构成新的数据集；

<img src="pictures/image-20240130202804147.png" alt="image-20240130202804147" style="zoom:33%;" />



#### ModelScopeT2V

论文地址：https://arxiv.org/abs/2308.06571

模型链接：https://huggingface.co/ali-vilab/text-to-video-ms-1.7b

该工作的创新点在于：

1. 模型结构上增加了时间维度依赖（时间维度的convelution和attention操作），来提升视频前后帧的连贯程度；
2. 同时使用“图-文”对和“视频-文”对，来训练模型；

<img src="pictures/image-20240219110620060.png" alt="image-20240219110620060" style="zoom:33%;" />

### VideoCrafter2

论文地址：https://arxiv.org/abs/2401.09047

项目地址：https://github.com/AILab-CVC/VideoCrafter

该工作在第一个版本的基础上，目标是解决如何使用低质量的视频数据来训练模型，且能让模型生成高质量的视频；

做法是，借鉴了ModelScopeT2V的思路，同时使用时间维度和空间维度的依赖，先训练一波模型。然后使用“合成的高质量图像数据”，固定时间维度模块，微调空间维度模块。



### DiTs

论文地址：https://arxiv.org/abs/2212.09748

参考链接：https://zhuanlan.zhihu.com/p/641013157

该工作的主要思想是：

1. 使用transformer替代常用的unet；
2. 发现模型越大，生成的效果越好；

<img src="pictures/image-20240219164007809.png" alt="image-20240219164007809" style="zoom: 50%;" />







## MoE（Mixture-of-Experts）

###### MoE 全局概览

参考链接：https://zhuanlan.zhihu.com/p/542465517

HuggingFace 论坛文章：https://huggingface.co/blog/moe  翻译地址：https://baoyu.io/translations/llm/mixture-of-experts-explained



###### Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer

论文链接：https://arxiv.org/abs/1701.06538

参考链接：https://zhuanlan.zhihu.com/p/335024684



###### Mixture-of-Experts with Expert Choice Routing

论文链接：https://arxiv.org/abs/2202.09368

<img src="pictures/image-20231220133754127.png" alt="image-20231220133754127" style="zoom:33%;" />

传统思想：每个Token选择最相关的K个Expert；

该文思想：每个Expert选择最相关的K个Tokens；

该文的效果如下：

<img src="pictures/image-20231220141110066.png" alt="image-20231220141110066" style="zoom:33%;" />

## 扩散模型

### DDPM

论文链接：https://arxiv.org/abs/2006.11239

项目链接：https://github.com/hojonathanho/diffusion

参考链接：https://zhuanlan.zhihu.com/p/563661713

参考实现：https://github.com/lucidrains/denoising-diffusion-pytorch

DDPM整个的推导逻辑十分复杂，可以直接看最终的优化目标。

DDPM整体分成两个部分，一个是扩散过程——不断往图像中添加噪声，直至其变成一个随机噪声；另一个是反向过程——即根据当前带噪的图像，不断去除其中的噪声，直至将其还原成原始的图像；

其优化目标，经过简化后，主要思路是使用模型来预测图像中的噪声；

<img src="pictures/image-20240226115855012.png" alt="image-20240226115855012" style="zoom: 25%;" />

从上图可以看到，训练阶段，采样一个原始图像后，为其添加噪声，然后训练模型根据添加噪声后的样本来估计实际添加的噪声；

生成阶段，根据模型生成的噪声，在样本上去除噪声的同事，还需要**额外累加一部分随机噪声**；

原论文中使用的Unet模型，结构如下图所示：

<img src="pictures/image-20240228173821436.png" alt="image-20240228173821436" style="zoom: 33%;" />

代码实现中，使用了[FID](https://www.jiqizhixin.com/articles/2019-10-14-13)来度量原图和生成图像之间的**分布相似性（在实际计算过程中，模型生成图像时的初始噪点并非和原图的对应）**，从而评估模型性能的好坏，值越低越好。

另外，代码中也采纳了一些trick来提升生成图像的效果：

1. [EMA](https://zhuanlan.zhihu.com/p/68748778)：原作者只在测试阶段使用该方法，可以提升测试的效果，并且让模型更加鲁棒；
2. 



### IDDPM

论文链接：https://arxiv.org/abs/2102.09672

参考链接：

论文在DDPM的基础上，做了如下改进：

1. [Cosine Schedule](https://arxiv.org/pdf/2102.09672.pdf)：探讨了原有的Linear Schedule在低分辨率的图像上，会过快地引入噪声；

<img src="pictures/image-20240304150534246.png" alt="image-20240304150534246" style="zoom: 50%;" />

2. 



### DDIM

论文链接：https://arxiv.org/abs/2010.02502

参考链接：https://zhuanlan.zhihu.com/p/565698027

**特点：训练过程和DDPM保持一致，生成过程通过子序列加速，并且可以保证生成是一个确定性过程**；

DDIM中，对于生成阶段，根据x_t生成x_{t-1}的公式可以格式化为：

<img src="pictures/image-20240301164239230.png" alt="image-20240301164239230" style="zoom:33%;" />

这边论文进一步定义：

<img src="pictures/image-20240301164613297.png" alt="image-20240301164613297" style="zoom:33%;" />

根据以上定义，如果 $\eta=1$ ，那么整个生成过程就和DDPM一样了；如果 $\eta=0$ ，那么生成过程中就不会添加随机噪音，变成了一个确定性的过程。

DDIM通过采样子序列进行生成加速：

<img src="pictures/image-20240301164718249.png" alt="image-20240301164718249" style="zoom:40%;" />

DDIM的训练流程和DDPM保持一致，在采样生成阶段，减少了生成样本过程迭代的轮数；

<img src="pictures/image-20240301164843090.png" alt="image-20240301164843090" style="zoom:33%;" />



### SDXL

论文链接：https://arxiv.org/abs/2307.01952

参考链接：https://zhuanlan.zhihu.com/p/643420260

官方项目链接：https://github.com/Stability-AI/generative-models

训练项目链接：https://github.com/Linaqruf/kohya-trainer



## 位置编码



### 旋转位置编码（RoPE）

<u>与传统的三角位置编码和自学习编码相比，RoPE编码是一种乘性位置编码方法，能够在Attention中表示相对位置，具有可解释性、良好的外推性质。可以使用PI位置插值或者NTK-RoPE的方法，进行位置编码的外推。</u>

公式如下所示，Q、V矩阵相乘后可以很好地表示相对位置距离；

<img src="pictures/image-20231229085614411.png" alt="image-20231229085614411" style="zoom:25%;" />

以上R矩阵是一个稀疏矩阵，故可以通过下面这种方法提高计算效率：

<img src="pictures/image-20231229085843417.png" alt="image-20231229085843417" style="zoom: 25%;" />

**源码学习 - ChatGLM-3：**

1. 在ChatGLM中，只对特征的前K维引入位置编码；
2. 注意，RoPE编码需要对Q和K两个向量引入位置编码信息；



**RoPE的优点：**

1. 一定的外推性
2. 计算简单，且可以表示相对位置
3. 与线性注意力机制兼容：为什么？



###### 【参考】十分钟读懂旋转编码（RoPE）

链接：https://zhuanlan.zhihu.com/p/647109286

###### 【参考】Transformer升级之路：2、博采众长的旋转式位置编码

链接：https://kexue.fm/archives/8265

###### 【参考】RoPE旋转位置编码深度解析：理论推导、代码实现、长度外推

链接：https://zhuanlan.zhihu.com/p/645263524

###### 【参考】如何理解 RoPE 的 NTK 扩展

链接：https://zhuanlan.zhihu.com/p/648701937

链接：https://zhuanlan.zhihu.com/p/675243992

链接：https://zhuanlan.zhihu.com/p/660073229

###### 【参考】Transformer升级之路：10、RoPE是一种β进制编码

链接：https://kexue.fm/archives/9675

###### 【论文】RoFormer: Enhanced Transformer with Rotary Position Embedding

链接：https://arxiv.org/abs/2104.09864



### Attention with Linear Biases (ALiBi)

思路非常直接，不是将位置编码作用在Embedding上，而是直接作用在Attention Score上。

<img src="pictures/image-20240102132524758.png" alt="image-20240102132524758" style="zoom:33%;" />

**ALiBi位置编码的优点：**

1. 计算更加简单
2. 外推性更好



###### 【参考】大模型位置编码-ALiBi位置编码

链接：https://zhuanlan.zhihu.com/p/656684326

###### 【论文】 Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation

链接：https://arxiv.org/abs/2108.12409



### 其他

##### 混合精度下的位置编码问题

###### 【参考】混合精度下位置编码竟有大坑，LLaMA等主流开源模型纷纷中招

链接：https://zhuanlan.zhihu.com/p/651588659